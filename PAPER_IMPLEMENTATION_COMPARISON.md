# Paper vs Implementation Comparison Report

## Executive Summary
This document compares the research paper "Predicting Heart Attack Risk: An Ensemble Modeling Approach" with the actual implementation to identify matches, discrepancies, and areas requiring improvement.

---

## ‚úÖ MATCHING ELEMENTS

### 1. Dataset & Basic Setup
- **Paper**: Kaggle "Cardio Train" dataset with 70,000 patient records
- **Implementation**: ‚úÖ MATCHES - Dataset confirmed in notebook (70,000 records, 32 columns)

### 2. Models Trained
- **Paper**: Logistic Regression, Random Forest, XGBoost, CatBoost, LightGBM
- **Implementation**: ‚úÖ MATCHES - All models trained in notebook (oneLastTime.ipynb)
- **Note**: `train_model.py` only trains XGBoost and CatBoost (simplified for deployment)

### 3. Performance Metrics (Paper Table II)
| Model | Paper Acc | Paper ROC-AUC | Implementation Status |
|-------|-----------|---------------|----------------------|
| Logistic Regression | 0.839 | 0.915 | ‚úÖ MATCHES (notebook) |
| Random Forest | 0.847 | 0.918 | ‚úÖ MATCHES (notebook) |
| XGBoost | 0.848 | 0.923 | ‚úÖ MATCHES (notebook) |
| CatBoost | 0.849 | 0.925 | ‚úÖ MATCHES (notebook) |
| LightGBM | 0.846 | 0.920 | ‚úÖ MATCHES (notebook) |
| Weighted Ensemble | 0.849 | 0.924 | ‚úÖ MATCHES (notebook) |

### 4. Feature Engineering
- **Paper**: Extended from 13 to 32 features
- **Implementation**: ‚úÖ MATCHES - 24 numeric + 4 categorical = 28 raw features ‚Üí 40 processed features (after one-hot encoding)

#### Key Derived Features (All Present):
- ‚úÖ BP_diff (Pulse Pressure): `ap_hi - ap_lo`
- ‚úÖ MAP (Mean Arterial Pressure): `(ap_hi + 2*ap_lo) / 3`
- ‚úÖ Pulse Pressure Ratio: `(ap_hi - ap_lo) / ap_hi`
- ‚úÖ Age Group: Categorical buckets (30-39, 40-49, 50-59, 60+)
- ‚úÖ Hypertension Flag: `1 if ap_hi >= 140 or ap_lo >= 90 else 0`
- ‚úÖ Obesity Flag: `1 if BMI >= 30 else 0`
- ‚úÖ Smoker Alcoholic: Binary flag
- ‚úÖ Protein Level: Present in implementation
- ‚úÖ Ejection Fraction: Present in implementation
- ‚úÖ Risk Level: Low/Medium/High categories

### 5. Preprocessing Pipeline
- **Paper**: StandardScaler for numeric, OneHotEncoder for categorical, median/mode imputation
- **Implementation**: ‚úÖ MATCHES - Exact same pipeline in `train_model.py`

### 6. Ensemble Strategy
- **Paper**: Weighted average ensemble `P_ensemble = w_X * P_XGBoost + w_C * P_CatBoost`
- **Implementation**: ‚úÖ MATCHES - Same formula in `app.py` (line 380)

### 7. SHAP Analysis
- **Paper**: SHAP summary plots and force plots for interpretability
- **Implementation**: ‚úÖ MATCHES - SHAP analysis present in notebook (Cells 58-59)

### 8. Streamlit Deployment
- **Paper**: Streamlit web interface for real-time prediction
- **Implementation**: ‚úÖ MATCHES - Full Streamlit app in `app.py`

---

## ‚ùå DISCREPANCIES & MISSING ELEMENTS

### 1. **CRITICAL: Hybrid Dual-Threshold Risk Mapping (Mapping A & B)**
- **Paper Claims**: 
  - Hybrid Mapping A: Treats moderate cases as "at-risk" (higher recall: 0.880)
  - Hybrid Mapping B: Treats moderate cases as "safe" (higher precision: 0.861)
  - Paper shows Figures 7 & 8 visualizing these mappings
- **Implementation**: ‚ùå **NOT FOUND** - Only single threshold (0.5) used in `app.py`
- **Impact**: High - This is a key contribution mentioned in the paper
- **Action Required**: Implement dual-threshold logic with configurable mappings

### 2. **CRITICAL: Ensemble Weight Optimization**
- **Paper Claims**: 
  - "We optimized the weights on a validation set (grid search over 0.0‚Äì1.0 in steps of 0.05) by maximizing ROC-AUC and F1 on the hold-out fold"
- **Implementation**: ‚ùå **NOT FOUND** - Hardcoded 50-50 weights in `ensemble_weights.json`
- **Impact**: High - Paper claims optimization was performed
- **Action Required**: Implement grid search for weight optimization or document that equal weights were used

### 3. **CRITICAL: Lifestyle Score Formula Mismatch**
- **Paper Formula**: `Lifestyle = active - (smoke + alco)` (scaled)
- **Implementation Formula**: `lifestyle_score = smoke + alco + (1 - active)` 
  - If smoke=1, score += 1
  - If alco=1, score += 1
  - If active=0, score += 1
- **Impact**: Medium - Different formulas yield different scores
- **Action Required**: Align implementation with paper formula or update paper

### 4. **CRITICAL: Risk Age Formula Mismatch**
- **Paper Formula**: `Risk_Age = age_years + BMI/5 + 2*(cholesterol > 1) + (gluc > 1)`
- **Implementation Formula**: `Risk_Age = age_years + (health_risk_score * 5)`
- **Impact**: Medium - Different formulas
- **Action Required**: Align formulas or document the difference

### 5. **MISSING: PR-AUC (Precision-Recall AUC)**
- **Paper Claims**: "ROC-AUC and PR-AUC" mentioned in evaluation metrics
- **Implementation**: ‚ùå **NOT FOUND** - Only ROC-AUC computed
- **Impact**: Low-Medium - Additional metric mentioned but not computed
- **Action Required**: Add PR-AUC calculation

### 6. **MISSING: Calibration Plots & Brier Scores**
- **Paper Claims**: "Calibration plots (reliability diagrams) and Brier scores to assess probability calibration"
- **Implementation**: ‚ùå **NOT FOUND** - No calibration analysis
- **Impact**: Medium - Important for healthcare applications
- **Action Required**: Add calibration analysis

### 7. **MISSING: Per-Group Metrics (Fairness Analysis)**
- **Paper Claims**: "Per-group metrics (age groups, gender, hypertension flag) to check fairness and performance across subpopulations"
- **Implementation**: ‚ùå **NOT FOUND** - No subgroup analysis
- **Impact**: Medium - Important for clinical validation
- **Action Required**: Add fairness metrics by subgroups

### 8. **MISSING: SHAP in Streamlit App**
- **Paper Claims**: "SHAP explanation snippet" in Streamlit output
- **Implementation**: ‚ùå **NOT FOUND** - SHAP only in notebook, not in `app.py`
- **Impact**: Medium - Paper claims real-time SHAP explanations in deployment
- **Action Required**: Integrate SHAP explanations into Streamlit app

### 9. **MISSING: Reason String Generation**
- **Paper Claims**: "Reason: A post-hoc human-readable string generated from thresholds (e.g., 'High BP, Low EF') for interpretability"
- **Implementation**: ‚ùå **NOT FOUND** - Reason column dropped but not generated in app
- **Impact**: Low-Medium - Mentioned as interpretability feature
- **Action Required**: Implement reason string generation in Streamlit app

### 10. **MISSING: Logistic Regression & Random Forest in Training Script**
- **Paper**: All 5 models trained and compared
- **Implementation**: `train_model.py` only trains XGBoost and CatBoost
- **Impact**: Low - Notebook has all models, but training script is incomplete
- **Action Required**: Add LR and RF to training script for reproducibility

### 11. **Minor: LightGBM Performance Discrepancy**
- **Paper Table II**: LightGBM Acc: 0.846, ROC-AUC: 0.920
- **Notebook Output**: LightGBM Acc: 0.848857, ROC-AUC: 0.923164
- **Impact**: Low - Small difference, likely due to rounding or different test sets
- **Action Required**: Verify if this is a rounding issue

---

## üîß IMPROVEMENTS NEEDED (Priority Order)

### HIGH PRIORITY (Must Fix for Paper Consistency)

1. **Implement Hybrid Dual-Threshold Risk Mapping**
   - Add Mapping A (moderate ‚Üí at-risk) for higher recall
   - Add Mapping B (moderate ‚Üí safe) for higher precision
   - Add UI toggle in Streamlit app
   - Update ensemble prediction logic

2. **Implement Ensemble Weight Optimization**
   - Add grid search over weights (0.0-1.0, steps of 0.05)
   - Optimize on validation set using ROC-AUC and F1
   - Save optimized weights
   - Document the optimization process

3. **Fix Lifestyle Score Formula**
   - Align with paper: `Lifestyle = active - (smoke + alco)`
   - Or update paper to match implementation
   - Document the choice

4. **Fix Risk Age Formula**
   - Align with paper: `Risk_Age = age_years + BMI/5 + 2*(cholesterol > 1) + (gluc > 1)`
   - Or update paper to match implementation

5. **Add SHAP to Streamlit App**
   - Integrate SHAP explanations for individual predictions
   - Show force plots or waterfall plots
   - Display top contributing features

### MEDIUM PRIORITY (Important for Completeness)

6. **Add Calibration Analysis**
   - Generate calibration plots (reliability diagrams)
   - Calculate Brier scores
   - Add to evaluation metrics

7. **Add Fairness Metrics**
   - Calculate metrics by age groups
   - Calculate metrics by gender
   - Calculate metrics by hypertension flag
   - Create fairness report

8. **Add PR-AUC Metric**
   - Calculate Precision-Recall AUC
   - Add to evaluation metrics table
   - Compare across models

9. **Implement Reason String Generation**
   - Generate human-readable explanations
   - Include in Streamlit app output
   - Based on feature thresholds

### LOW PRIORITY (Nice to Have)

10. **Complete Training Script**
    - Add Logistic Regression training
    - Add Random Forest training
    - Add LightGBM training
    - Generate complete metrics table

11. **Documentation Updates**
    - Update README with all features
    - Document formula choices
    - Add reproducibility instructions

---

## üìä SUMMARY STATISTICS

- **Total Paper Claims**: ~25 major claims
- **Matches**: 18 (72%)
- **Discrepancies**: 4 (16%)
- **Missing Elements**: 7 (28%)
- **Critical Issues**: 4
- **Medium Issues**: 5
- **Low Issues**: 2

---

## üéØ RECOMMENDATIONS

### For Paper Submission:
1. **Fix Critical Discrepancies**: Address hybrid mapping, weight optimization, and formula mismatches
2. **Add Missing Analyses**: Include calibration, fairness, and PR-AUC metrics
3. **Update Methodology Section**: Clarify which formulas were actually used
4. **Update Results Section**: Include all promised metrics and visualizations

### For Implementation:
1. **Implement Hybrid Mappings**: Add dual-threshold logic to match paper
2. **Add Weight Optimization**: Implement grid search or document why equal weights were used
3. **Integrate SHAP**: Add SHAP explanations to Streamlit app
4. **Complete Evaluation**: Add calibration, fairness, and PR-AUC analyses
5. **Fix Formulas**: Align Lifestyle Score and Risk Age formulas with paper

### For Code Quality:
1. **Complete Training Script**: Add all models to `train_model.py`
2. **Add Tests**: Create unit tests for feature engineering formulas
3. **Documentation**: Add docstrings and comments explaining formula choices
4. **Reproducibility**: Ensure all random seeds are set and documented

---

## üìù NOTES

- The notebook (`oneLastTime.ipynb`) contains most of the analysis mentioned in the paper
- The training script (`train_model.py`) is simplified for deployment and only trains the ensemble models
- The Streamlit app (`app.py`) is functional but missing some interpretability features mentioned in the paper
- Most performance metrics match the paper, indicating the models were trained correctly
- The main gaps are in evaluation completeness and interpretability features in the deployed app

---

**Last Updated**: [Current Date]
**Review Status**: Ready for Implementation Fixes


